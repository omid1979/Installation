TCPTuning

TCPTuning

در این مقاله میخواهیم با قابلیت های کرنل لینوکس برای عملکرد شبکه آشنا بشویم.سرورها از همان stack TCP/IP استفاده می کنند که توسط کلاینت هایشان استفاده می شود. به همین دلیل، مقادیر پیش‌فرض برای استفاده‌های عمومی پیکربندی شده‌اند و برای محیط‌های سرور پربار بهینه نشده‌اند.

نکته حائز اهمیت قبل از اینکه این عملیاتها را در محیط اصلی انجام دهید پیشنهاد میشود در محیط تستی انجام دهید.

با در نظر گرفتن این موضوع، در اینجا یک مرور سریع از مراحل انجام شده در طول انتقال و دریافت داده ها آورده شده است:

1.  برنامه ابتدا داده ها را در یک سوکت می نویسد که به نوبه خود در بافر انتقال قرار می گیرد.
2.  کرنل داده ها را در یک واحد داده پروتکل PDU کپسوله می کند.
3.  سپس PDU به صف ارسال هر دستگاه منتقل می شود.
4.  سپس درایور NIC PDU را از صف ارسال خارج می کند و آن را در NIC کپی می کند.
5.  NIC داده ها را ارسال می کند و یک وقفه سخت افزاری ایجاد می کند.
6.  در انتهای دیگر کانال ارتباطی، NIC فریم را دریافت می کند، آن را روی بافر دریافت کپی می کند و یک وقفه سخت ایجاد می کند.
7.  کرنل به نوبه خود وقفه را مدیریت می کند و یک وقفه نرم برای پردازش بسته ایجاد می کند.
8.  در نهایت کرنل وقفه نرم را مدیریت می کند و بسته را به سمت پشته TCP/IP برای decapsulation حرکت می دهد و آن را در یک بافر دریافت قرار می دهد تا فرآیندی از آن خوانده شود.

![a85b5de3140a9fd39f3a7af358a5b233.png](:/4adf1223c30343edbe13f80452688812)

نحوه برخورد سیستم عامل شما با داده ها

داده هایی که برای یک سیستم خاص ارسال می شوند ابتدا توسط NIC دریافت می شوند و در بافر حلقه گیرنده (RX) موجود در NIC که دارای TX (برای انتقال داده) نیز می باشد، ذخیره می شود. هنگامی که بسته در دسترس هسته قرار گرفت، درایور دستگاه softirq (وقفه نرم افزاری) را افزایش می دهد که باعث می شود DMA (دسترسی به حافظه داده) سیستم آن بسته را به هسته لینوکس ارسال کند. داده های بسته در هسته لینوکس در ساختار داده sk_buff ذخیره می شود تا بسته را تا MTU (حداکثر واحد انتقال) نگه دارد. هنگامی که تمام بسته ها در بافر هسته پر می شوند، به لایه پردازش بالایی - IP، TCP یا UDP ارسال می شوند. سپس داده ها در فرآیند دریافت داده ترجیحی کپی می شوند.

![370125e4f968d1244d3ed21a4e792b37.png](:/4c3d47dced8b419aa185d8dabec5a236)

تنظیم آداپتور شبکه (NIC) **Tuning the network adapter (NIC)**

فریم های جامبو: همانطور که با دستور ip addr نشان داده شده است، یک فریم به طور پیش فرض می تواند حداکثر 1500 بایت را حمل کند.

enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000

link/ether 08:00:27:5d:4e:57 brd ff:ff:ff:ff:ff:ff

inet 192.168.43.172/24 brd 192.168.43.255 scope global enp0s3

valid_lft forever preferred_lft forever

در اینجا، برای eth0 که رابط شبکه پیش‌فرض برای اترنت است، مقدار MTU (حداکثر واحد انتقال) 1500 تنظیم می‌شود. این مقدار برای یک اسلات تعریف می‌شود. اتصالات اترنت 10 گیگابیت بر ثانیه یا بیشتر ممکن است نیاز به افزایش مقدار به 9000 داشته باشند. این فریم های بزرگ را فریم جامبو می نامند. برای اینکه اترنت از فریم های جامبو استفاده کند، می توانید از ifconfig به صورت زیر استفاده کنید:

$ifconfig eth0 mtu 9000

$ifconfig eth0 mtu 9000

ادغام وقفه (IC) **Interrupt Coalescence (IC)**

پس از اینکه آداپتور شبکه بسته ها را دریافت کرد، درایور دستگاه یک وقفه سخت و به دنبال آن وقفه های نرم (که توسط NAPI مدیریت می شود) صادر می کند. ادغام وقفه تعداد بسته هایی است که آداپتور شبکه قبل از صدور وقفه سخت دریافت می کند. تغییر مقدار برای ایجاد وقفه سریع می تواند منجر به سربار زیاد و در نتیجه کاهش عملکرد شود، اما داشتن مقدار بالا ممکن است منجر به از دست دادن بسته شود. به‌طور پیش‌فرض، تنظیمات شما در حالت IC تطبیقی است که به صورت خودکار مقدار را بر اساس ترافیک شبکه متعادل می‌کند. اما از آنجایی که هسته‌های جدید از NAPI استفاده می‌کنند، که باعث می‌شود وقفه‌های سریع بسیار کم‌هزینه‌تر شوند (از لحاظ عملکرد)، می‌توانید این ویژگی را غیرفعال کنید.

$ ethtool -c eth0

برای غیرفعال کردن آن از دستور زیر استفاده کنید:

$ethtool -C eth0 adaptive-rx off

نکته مهم : اگر سیستم شما مانند یک وب سرور میزبان ترافیک بالایی ایجاد می کند، نباید آن را غیرفعال کنید.

  
<br/>

برای ایجاد تغییرات به صورت دائم در تنظیمات کرنل، ورودی ها را به فایل /etc/sysctl.conf اضافه کنید و سپس “sysctl -p” را برای اعمال اجرا کنید.

مانند همه سیستم عامل ها، حداکثر اندازه بافر TCP لینوکس بسیار کوچک است. من پیشنهاد می کنم آنها را به تنظیمات زیر تغییر دهید:

برای افزایش حداکثر اندازه بافر TCP قابل تنظیم با استفاده از setsockopt():

sysctl -w net.core.rmem_max=2147483647

sysctl -w net.core.wmem_max=2147483647

برای تنظیم خودکار بافر TCP در لینوکس به صورت زیر محدود می‌کند :

حداکثر 16 مگابایت برای 1GE

2 مگابایت یا 54 مگابایت برای 10 GE

sysctl -w net.ipv4.tcp_rmem=‘4096 87380 2147483647’

sysctl -w net.ipv4.tcp_wmem=‘4096 65536 2147483647’

استفاده از netstat -s | grep -i برای رویدادهای “xxx بار صف شنیدن یک سوکت سرریز” به مانیتور گوش می دهم.

همچنین باید بررسی کنید که موارد زیر روی مقدار پیش‌فرض 1 تنظیم شده باشند:

sysctl net.ipv4.tcp_window_scaling

sysctl net.ipv4.tcp_timestamps

sysctl net.ipv4.tcp_sack

توجه: باید tcp_mem را به صورت پیش فرض قرار دهید.

یکی دیگر از کارهایی که می توانید برای کمک به افزایش توان TCP با کارت های شبکه 1 گیگابایتی انجام دهید، افزایش اندازه صف Interface است. برای مسیرهایی با RTT بیش از 50 میلی ثانیه، مقدار 5000-10000 توصیه می شود. برای افزایش txqueuelen موارد زیر را انجام دهید:

\[root@server1 ~\] ifconfig eth0 txqueuelen 5000

نکته : با انجام این کار در برخی از مسیرهای طولانی و سریع، می توانید پهنای باند را تا 10 برابر افزایش دهید. این فقط برای هاست های متصل به اترنت گیگابیت ایده خوبی است و ممکن است عوارض جانبی دیگری مانند اشتراک گذاری نامساعدی بین جریان های متعدد داشته باشد.

سایر تنظیمات کرنل که به عملکرد کلی سرور در مورد ترافیک شبکه کمک می کند موارد زیر است:

TCP_FIN_TIMEOUT

این تنظیم زمانی را تعیین می کند که باید قبل از اینکه TCP/IP بتواند یک اتصال بسته را آزاد کند و از منابع آن استفاده مجدد کند، سپری شود. در این حالت TIME_WAIT، باز کردن مجدد اتصال به مشتری هزینه کمتری نسبت به برقراری اتصال جدید دارد. با کاهش مقدار این ورودی، TCP/IP می تواند اتصالات بسته را سریعتر آزاد کند و منابع بیشتری را برای اتصالات جدید در دسترس قرار دهد. با وجود بسیاری از اتصالات در حالت TIME_WAIT، این را اضافه کنید:

\[root@server:~\]# echo 30 > /proc/sys/net/ipv4/tcp_fin_timeout

TCP_KEEPALIVE_INTERVAL

این زمان انتظار بین پروب های فاصله ای isAlive را تعیین می کند. برای تنظیم:

\[root@server:~\]# echo 30 > /proc/sys/net/ipv4/tcp_keepalive_intvl

TCP_KEEPALIVE_PROBES

این تعداد پروب ها را قبل از اتمام زمان تعیین می کند. برای تنظیم:

\[root@server:~\]# echo 5 > /proc/sys/net/ipv4/tcp_keepalive_probes

TCP_TW_RECYCLE

این امکان بازیافت سریع سوکت های TIME_WAIT را فراهم می کند. مقدار پیش فرض 0 (غیرفعال) است. باید با احتیاط با لودبالانسرها استفاده شود.

\[root@server:~\]# echo 1 > /proc/sys/net/ipv4/tcp_tw_recycle

TCP_TW_REUSE

این امکان استفاده مجدد از سوکت‌ها را در حالت TIME_WAIT برای اتصالات جدید در شرایطی که از نظر پروتکل ایمن است، می‌دهد. مقدار پیش فرض 0 (غیرفعال) است. به طور کلی جایگزین امن تری برای tcp_tw_recycle است.

\[root@server:~\]# echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse

توجه: تنظیم tcp_tw_reuse مخصوصاً در محیط‌هایی که تعداد زیادی اتصال کوتاه باز هستند و در حالت TIME_WAIT باقی می‌مانند، مانند سرورهای وب و loadbalancer مفید است. استفاده مجدد از سوکت ها می تواند در کاهش بار سرور بسیار موثر باشد.

لینوکس با شروع در کرنل 2.6.7 (و به نسخه 2.4.27 پس‌پورت شده)، شامل الگوریتم‌های کنترل تراکم جایگزین در کنار الگوریتم سنتی «reno» می‌شود. اینها برای بازیابی سریع از دست دادن بسته در شبکه های WAN پرسرعت طراحی شده اند.

چند تنظیمات sysctl اضافی برای هسته های 2.6 و جدیدتر وجود دارد:

Not to cache ssthresh from previous connection:

net.ipv4.tcp_no_metrics_save = 1

To increase this for 10G NICS:

net.core.netdev_max_backlog = 30000

اگر سرور بارگیری دارید که دارای اتصالات زیادی در حالت TIME_WAIT است، فاصله زمانی TIME_WAIT را کاهش دهید که تعیین کننده زمانی است که باید قبل از اینکه TCP/IP بتواند یک اتصال بسته را آزاد کند و از منابع آن استفاده مجدد کند، سپری شود. این فاصله بین بسته شدن و رهاسازی به عنوان حالت TIME_WAIT یا دو برابر حداکثر طول عمر قطعه (2MSL) شناخته می شود. در طی این مدت، باز کردن مجدد اتصال به مشتری و سرور هزینه کمتری نسبت به ایجاد یک اتصال جدید دارد. با کاهش مقدار این ورودی، TCP/IP می تواند اتصالات بسته را سریعتر آزاد کند و منابع بیشتری برای اتصالات جدید فراهم کند. اگر برنامه در حال اجرا به انتشار سریع، ایجاد اتصالات جدید و توان عملیاتی کم به دلیل بسیاری از اتصالات در حالت TIME_WAIT نیاز دارد، این پارامتر را تنظیم کنید:

\[root@host1 ~\]# echo 5 > /proc/sys/net/ipv4/tcp_fin_timeout

همچنین می‌خواهم به این نکته اشاره کنم که داشتن تعداد کافی file descriptors در دسترس چقدر مهم است، زیرا تقریباً همه چیز در لینوکس یک فایل است.

برای بررسی حداکثر و در دسترس بودن فعلی خود، موارد زیر را اجرا کنید:

\[root@host1 ~\]# sysctl fs.file-nr

اولین مقدار (197600) تعداد دسته های فایل اختصاص داده شده است.

مقدار دوم (0) تعداد دسته های فایل استفاده نشده اما تخصیص یافته است. و مقدار سوم (3624009) حداکثر تعداد دسته فایل در سراسر سیستم است. با تنظیم پارامتر هسته زیر می توان آن را افزایش داد:

\[root@host1 ~\]# echo 10000000 > /proc/sys/fs/file-max

برای مشاهده تعداد file descriptors توسط یک فرآیند، می‌توانید از یکی از موارد زیر استفاده کنید:

The 28290 number is the process id.

\[root@host1 ~\]# lsof -a -p 28290

\[root@host1 ~\]# ls -l /proc/28290/fd | wc -l

&nbsp;

برای تنظیم Stack TCP مراحل زیر را دنبال کنید:

رفتار کرنل لینوکس را می توان با کمک پارامترهای مختلف تنظیم کرد. اینها گزینه هایی هستند که به منظور کنترل جنبه های مختلف سیستم به کرنل منتقل می شوند. این پارامترها را می توان در حین کامپایل کردن کرنل در زمان بوت یا در زمان اجرا با استفاده از سیستم فایل proc / و ابزارهایی مانند sysctl ارسال کرد.

حداکثر محدودیت فایل های باز را تنظیم کنید:

**$ ulimit -n # check existing limits for logged in user**

**\# ulimit -n 65535 # root change values above hard limits**

برای تنظیم دائمی محدودیت برای یک کاربر، /etc/security/limits.conf را باز کنید و خطوط زیر را در انتهای فایل اضافه کنید.

**&lt;username&gt; soft nofile &lt;value&gt; # soft limits**

**&lt;username&gt; hard nofile &lt;value&gt; # hard limits**

مشاهده تمام پارامترهای موجود:

**\# sysctl -a**

تنظیم خواندن و نوشتن پیش فرض روی TCP :

**\# echo ‘net.core.rmem_default=65536’ >> /etc/sysctl.conf**

**\# echo ‘net.core.wmem_default=65536’ >> /etc/sysctl.conf**

افزایش پارامتر **tcp_max_orphans**:

**\# echo ‘net.ipv4.tcp_max_orphans=4096’ >> /etc/sysctl.conf**

غیر فعال سازی مقدار slow start after idle:

**\# echo ‘net.ipv4.tcp_slow_start_after_idle=0’ >> /etc/sysctl.conf**

به حداقل رساندن اتصالهای مجدد به TCP:

**\# echo ‘net.ipv4.tcp_synack_retries=3’ >> /etc/sysctl.conf**

**\# echo ‘net.ipv4.tcp_syn_retries =3’ >> /etc/sysctl.conf**

مقدار دهی پارامتر **tcp_window_scaling**:

**\# echo ‘net.ipv4.tcp_window_scaling=1’ >> /etc/sysctl.conf**

فعال کردن timestamps :

**\# echo ‘net.ipv4.tcp_timestamp=1’ >> /etc/sysctl.conf**

فعال کردن تایید های acknowledgements :

**\# echo ‘net.ipv4.tcp_sack=0’ >> /etc/sysctl.conf**

ارسال داده در حالت **fastopen** :

**\# echo ‘net.ipv4.tcp_fastopen=1’ >> /etc/sysctl.conf**

مقدار دهی تعداد کانکشن هایی باز که قبل از دریافت ack باید حفظ گردد:

**\# echo ‘tcp_max_syn_backlog=1500’ >> /etc/sysctl.conf**

مقدار دهی تعداد درخواست های tcp keepalive قبل از ارسال جهت تصمیم گیری برای فهم خطای کانکشن:

**\# echo ‘tcp_keepalive_probes=5’ >> /etc/sysctl.conf**

مقدار دهی زمان keep-alive که مشخص کننده زمان کانکشن بعد از kill شدن می باشد

**\# echo ‘tcp_keepalive_time=1800’ >> /etc/sysctl.conf**

مقدار دهی بازه ارسال پکت keep-alive :

**\# echo ‘tcp_keepalive_intvl=60’ >> /etc/sysctl.conf**

مقدار دهی پارامتر های reusee , recycle در حالت wait کانکشن:

**\# echo ‘net.ipv4.tcp_tw_reuse=1’ >> /etc/sysctl.conf**

**\# echo ‘net.ipv4.tcp_tw_recycle=1’ >> /etc/sysctl.conf**

افزایش حداکثر تعداد کانکشن ها:

**\# echo ‘net.ipv4.ip_local_port_range=32768 65535’ >> /etc/sysctl.conf**

مقدار دهی TCP_FIN_timeout:

**\# echo ‘tcp_fin_timeout=60’ >> /etc/sysctl.conf**

id: b5443c6c90844f90bb38249f54b67aa7
parent_id: 9051567a85b34643b4e0252a8fdf0ac0
created_time: 2024-09-15T08:22:32.860Z
updated_time: 2024-09-15T08:23:15.762Z
is_conflict: 0
latitude: 35.68919750
longitude: 51.38897360
altitude: 0.0000
author: 
source_url: 
is_todo: 0
todo_due: 0
todo_completed: 0
source: joplin-desktop
source_application: net.cozic.joplin-desktop
application_data: 
order: 0
user_created_time: 2024-09-15T08:22:32.860Z
user_updated_time: 2024-09-15T08:23:15.762Z
encryption_cipher_text: 
encryption_applied: 0
markup_language: 1
is_shared: 0
share_id: 
conflict_original_id: 
master_key_id: 
user_data: 
deleted_time: 0
type_: 1